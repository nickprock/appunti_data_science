{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"w9HrgIUGsMul"},"source":["# Transforming\n","\n","In this notebook, we will explore how to use Large Language Models for text transformation tasks such as language translation, spelling and grammar checking, tone adjustment, and format conversion.\n","\n","## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"hwla67OMsPvS"},"outputs":[],"source":["import yaml\n","\n","with open(\"config.yml\") as f:\n","    config = yaml.load(f, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import openai\n","import cohere\n","openai.api_key  = config['openai_key']\n","co = cohere.Client(config['cohere_key'])"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sVLc2wgxsVt3"},"outputs":[],"source":["def get_completion(prompt, model=\"gpt-3.5-turbo\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=messages,\n","        temperature=0, # this is the degree of randomness of the model's output\n","    )\n","    return response.choices[0].message[\"content\"]\n","\n","def cohere_complete(prompt, model= \"command-nightly\"):\n","    response = co.generate(\n","        model=model,\n","        prompt=prompt,\n","        max_tokens=2500,\n","        temperature=0\n","    )\n","    return response.generations[0].text"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ifo15m42sXnP"},"source":["## Translation\n","\n","ChatGPT is trained with sources in many languages. This gives the model the ability to do translation. Here are some examples of how to use this capability."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fruG0Lm0saOI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hola, me gustaría ordenar una licuadora.\n","\n","\n","Hola, me gustaría ordenar un licuador.\n"]}],"source":["prompt = f\"\"\"\n","Translate the following English text to Spanish: \\ \n","```Hi, I would like to order a blender```\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GB_Ijm9Hsc3m"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is French.\n","\n","\n","The answer is French\n"]}],"source":["prompt = f\"\"\"\n","Tell me which language this is: \n","```Combien coûte le lampadaire?```\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RlG2p2rKsfMk"},"outputs":[{"name":"stdout","output_type":"stream","text":["French pirate: ```Je veux commander un ballon de basket```\n","Spanish pirate: ```Quiero pedir una pelota de baloncesto```\n","English pirate: ```I want to order a basketball```\n","\n","\n","French: Je voudrais commander un basketball\n","Spanish: Me gustaría ordenar un basketball\n","English: I want to order a basketball\n"]}],"source":["prompt = f\"\"\"\n","Translate the following  text to French and Spanish\n","and English pirate: \\\n","```I want to order a basketball```\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vacFPFnjtMfB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Formal: ¿Le gustaría ordenar una almohada?\n","Informal: ¿Te gustaría ordenar una almohada?\n","\n","\n","The formal translation of the text is: \n","¿Crees en ordenar una almohada?\n","\n","The informal translation of the text is: \n","¿Quieres ordenar una almohada?\n"]}],"source":["prompt = f\"\"\"\n","Translate the following text to Spanish in both the \\\n","formal and informal forms: \n","'Would you like to order a pillow?'\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ppknHQNOtPtQ"},"source":["### Universal Translator\n","Imagine you are in charge of IT at a large multinational e-commerce company. Users are messaging you with IT issues in all their native languages. Your staff is from all over the world and speaks only their native languages. You need a universal translator!"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CuJu5reQtR3P"},"outputs":[],"source":["user_messages = [\n","  \"La performance du système est plus lente que d'habitude.\",  # System performance is slower than normal         \n","  \"Mi monitor tiene píxeles que no se iluminan.\",              # My monitor has pixels that are not lighting\n","  \"Il mio mouse non funziona\",                                 # My mouse is not working\n","  \"Mój klawisz Ctrl jest zepsuty\",                             # My keyboard has a broken control key\n","  \"我的屏幕在闪烁\"                                               # My screen is flashing\n","] "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ijxUZH6Ztsuw"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original message (This is French.): La performance du système est plus lente que d'habitude.\n","English: The system performance is slower than usual.\n","Korean: 시스템 성능이 평소보다 느립니다.\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n"]},{"name":"stdout","output_type":"stream","text":["The performance of the system is slower than usual.\n","\n","```안녕하세요? 요리책 잘 모두 받았습니다.```\n","\n","I got all the recipes.\n","Original message (This is Spanish.): Mi monitor tiene píxeles que no se iluminan.\n"]},{"ename":"RateLimitError","evalue":"Rate limit reached for default-gpt-3.5-turbo in organization org-9YxN6f621GLcigIQBYHmnkQy on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal message (\u001b[39m\u001b[39m{\u001b[39;00mlang\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00missue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mTranslate the following  text to English \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mand Korean: ```\u001b[39m\u001b[39m{\u001b[39;00missue\u001b[39m}\u001b[39;00m\u001b[39m```\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m response_openai \u001b[39m=\u001b[39m get_completion(prompt)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(response_openai)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion\u001b[39m(prompt, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m): \u001b[39m# Andrew mentioned that the prompt/ completion paradigm is preferable for this class\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[0;32m----> 3\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      4\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      5\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m      6\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-9YxN6f621GLcigIQBYHmnkQy on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."]}],"source":["for issue in user_messages:\n","    prompt = f\"Tell me what language this is: ```{issue}```\"\n","    lang = get_completion(prompt)\n","    print(f\"Original message ({lang}): {issue}\")\n","\n","    prompt = f\"\"\"\n","    Translate the following  text to English \\\n","    and Korean: ```{issue}```\n","    \"\"\"\n","    response_openai = get_completion(prompt)\n","    print(response_openai)\n","\n","    print(\"\\n\")\n","\n","    response_cohere = cohere_complete(prompt)\n","    print(response_cohere)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aOs_3X34tvnG"},"source":["## Try it yourself!\n","Try some translations on your own!"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"l_Dv7fUDtx9F"},"source":["## Tone Transformation\n","Writing can vary based on the intended audience. ChatGPT can produce different tones.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"pvXGJddct1YG"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dear Sir/Madam,\n","\n","I am writing to bring to your attention a standing lamp that I believe may be of interest to you. Please find attached the specifications for your review.\n","\n","Thank you for your time and consideration.\n","\n","Sincerely,\n","\n","Joe\n","\n","\n","Dear Sir, \n","\n","I am writing to inform you of a potential opportunity to purchase a standing lamp. The lamp has a unique design and would be a great addition to your inventory.\n","\n","Please let me know if you are interested in purchasing the lamp and I will be happy to provide you with more information.\n","\n","Thank you,\n","\n","Joe\n"]}],"source":["prompt = f\"\"\"\n","Translate the following from slang to a business letter: \n","'Dude, This is Joe, check out this spec on this standing lamp.'\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0zwp3uOXuGOr"},"source":["## Format Conversion\n","ChatGPT can translate between formats. The prompt should describe the input and output formats."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"r6TfJDRXuH4S"},"outputs":[{"name":"stdout","output_type":"stream","text":["<table>\n","  <caption>Restaurant Employees</caption>\n","  <thead>\n","    <tr>\n","      <th>Name</th>\n","      <th>Email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>Shyam</td>\n","      <td>shyamjaiswal@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Bob</td>\n","      <td>bob32@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Jai</td>\n","      <td>jai87@gmail.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n","\n","\n","```html\n","<table>\n","    <tr>\n","        <th>Name</th>\n","        <th>Email</th>\n","    </tr>\n","    <tr>\n","        <td>Shyam</td>\n","        <td>shyamjaiswal@gmail.com</td>\n","    </tr>\n","    <tr>\n","        <td>Bob</td>\n","        <td>bob32@gmail.com</td>\n","    </tr>\n","    <tr>\n","        <td>Jai</td>\n","        <td>jai87@gmail.com</td>\n","    </tr>\n","</table>\n","```\n"]}],"source":["data_json = { \"resturant employees\" :[ \n","    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n","    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n","    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n","]}\n","\n","prompt = f\"\"\"\n","Translate the following python dictionary from JSON to an HTML \\\n","table with column headers and title: {data_json}\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"KJszkJLiuJ3-"},"outputs":[{"name":"stdout","output_type":"stream","text":["<table>\n","  <caption>Restaurant Employees</caption>\n","  <thead>\n","    <tr>\n","      <th>Name</th>\n","      <th>Email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>Shyam</td>\n","      <td>shyamjaiswal@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Bob</td>\n","      <td>bob32@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Jai</td>\n","      <td>jai87@gmail.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]}],"source":["from IPython.display import display, Markdown, Latex, HTML, JSON\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WfKCzlMAuNLv"},"source":["## Spellcheck/Grammar check.\n","\n","Here are some examples of common grammar and spelling problems and the LLM's response. \n","\n","To signal to the LLM that you want it to proofread your text, you instruct the model to 'proofread' or 'proofread and correct'."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"i1dh2LOKuOzi"},"outputs":[{"name":"stdout","output_type":"stream","text":["The girl with the black and white puppies has a ball.\n","\n","\n","\n","The girl with the black and white puppies has a ball.\n","No errors found.\n","\n","\n","\n","Yolanda has her notebook.\n","It's going to be a long day. Does the car need its oil changed?\n","\n","\n","\n","It's going to be a long day. Does the car need its oil changed?\n"]},{"ename":"RateLimitError","evalue":"Rate limit reached for default-gpt-3.5-turbo in organization org-9YxN6f621GLcigIQBYHmnkQy on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m text:\n\u001b[1;32m     11\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mProofread and correct the following text\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m    and rewrite the corrected version. If you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m    and errors, just say \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo errors found\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt use \u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39m    any punctuation around the text:\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39m    ```\u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m```\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     response_openai \u001b[39m=\u001b[39m get_completion(prompt)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(response_openai)\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion\u001b[39m(prompt, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m): \u001b[39m# Andrew mentioned that the prompt/ completion paradigm is preferable for this class\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[0;32m----> 3\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      4\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      5\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m      6\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/llm_test/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-9YxN6f621GLcigIQBYHmnkQy on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."]}],"source":["text = [ \n","  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n","  \"Yolanda has her notebook.\", # ok\n","  \"Its going to be a long day. Does the car need it’s oil changed?\",  # Homonyms\n","  \"Their goes my freedom. There going to bring they’re suitcases.\",  # Homonyms\n","  \"Your going to need you’re notebook.\",  # Homonyms\n","  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n","  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n","]\n","for t in text:\n","    prompt = f\"\"\"Proofread and correct the following text\n","    and rewrite the corrected version. If you don't find\n","    and errors, just say \"No errors found\". Don't use \n","    any punctuation around the text:\n","    ```{t}```\"\"\"\n","    response_openai = get_completion(prompt)\n","    print(response_openai)\n","\n","    print(\"\\n\")\n","\n","    response_cohere = cohere_complete(prompt)\n","    print(response_cohere)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"IsaM7NQVuW95"},"outputs":[{"name":"stdout","output_type":"stream","text":["I got this for my daughter's birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it. I think there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n","\n","\n","\n","Here is the corrected review:\n","\n","Got this for my daughter for her birthday because she keeps taking mine from my room.  Yes, adults also like pandas too.  She takes it everywhere with her, and it's super soft and cute.  One of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical.  It's a bit small for what I paid for it though.  I think there might be other options that are bigger for the same price.  It arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n"]}],"source":["text = f\"\"\"\n","Got this for my daughter for her birthday cuz she keeps taking \\\n","mine from my room.  Yes, adults also like pandas too.  She takes \\\n","it everywhere with her, and it's super soft and cute.  One of the \\\n","ears is a bit lower than the other, and I don't think that was \\\n","designed to be asymmetrical. It's a bit small for what I paid for it \\\n","though. I think there might be other options that are bigger for \\\n","the same price.  It arrived a day earlier than expected, so I got \\\n","to play with it myself before I gave it to my daughter.\n","\"\"\"\n","prompt = f\"proofread and correct this review: ```{text}```\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting redlines\n","  Downloading redlines-0.3.0-py3-none-any.whl (5.5 kB)\n","Installing collected packages: redlines\n","Successfully installed redlines-0.3.0\n"]}],"source":["!pip install redlines"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"uHIT579uuZD5"},"outputs":[{"data":{"text/markdown":["<span style=\"color:red;font-weight:700;text-decoration:line-through;\">Got </span><span style=\"color:red;font-weight:700;\">I got </span>this for my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">daughter for her </span><span style=\"color:red;font-weight:700;\">daughter's </span>birthday <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cuz </span><span style=\"color:red;font-weight:700;\">because </span>she keeps taking mine from my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">room.  </span><span style=\"color:red;font-weight:700;\">room. </span>Yes, adults also like pandas <span style=\"color:red;font-weight:700;text-decoration:line-through;\">too.  </span><span style=\"color:red;font-weight:700;\">too. </span>She takes it everywhere with her, and it's super soft and <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cute.  One </span><span style=\"color:red;font-weight:700;\">cute. However, one </span>of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. <span style=\"color:red;font-weight:700;text-decoration:line-through;\">It's </span><span style=\"color:red;font-weight:700;\">Additionally, it's </span>a bit small for what I paid for <span style=\"color:red;font-weight:700;text-decoration:line-through;\">it though. </span><span style=\"color:red;font-weight:700;\">it. </span>I think there might be other options that are bigger for the same <span style=\"color:red;font-weight:700;text-decoration:line-through;\">price.  It </span><span style=\"color:red;font-weight:700;\">price. On the positive side, it </span>arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/markdown":["<span style=\"color:red;font-weight:700;\">Here is the corrected review: </span>\n","\n","<span style=\"color:red;font-weight:700;\"></span>Got this for my daughter for her birthday <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cuz </span><span style=\"color:red;font-weight:700;\">because </span>she keeps taking mine from my room.  Yes, adults also like pandas too.  She takes it everywhere with her, and it's super soft and cute.  One of the ears is a bit lower than the other, and I don't think that was designed to be <span style=\"color:red;font-weight:700;text-decoration:line-through;\">asymmetrical. </span><span style=\"color:red;font-weight:700;\">asymmetrical.  </span>It's a bit small for what I paid for it <span style=\"color:red;font-weight:700;text-decoration:line-through;\">though. </span><span style=\"color:red;font-weight:700;\">though.  </span>I think there might be other options that are bigger for the same price.  It arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["from redlines import Redlines\n","\n","diff = Redlines(text,response_openai)\n","display(Markdown(diff.output_markdown))\n","\n","print(\"\\n\")\n","\n","diff = Redlines(text,response_cohere)\n","display(Markdown(diff.output_markdown))"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"pxN0ldJlua7Z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Title: A Soft and Cute Panda Plush Toy for All Ages\n","\n","Introduction:\n","As a parent, finding the perfect gift for your child's birthday can be a daunting task. However, I stumbled upon a soft and cute panda plush toy that not only made my daughter happy but also brought joy to me as an adult. In this review, I will share my experience with this product and provide an honest assessment of its features.\n","\n","Product Description:\n","The panda plush toy is made of high-quality materials that make it super soft and cuddly. Its cute design is perfect for children and adults alike, making it a versatile gift option. The toy is small enough to carry around, making it an ideal companion for your child on their adventures.\n","\n","Pros:\n","The panda plush toy is incredibly soft and cute, making it an excellent gift for children and adults. Its small size makes it easy to carry around, and its design is perfect for snuggling. The toy arrived a day earlier than expected, which was a pleasant surprise.\n","\n","Cons:\n","One of the ears is a bit lower than the other, which makes the toy asymmetrical. Additionally, the toy is a bit small for its price, and there might be other options that are bigger for the same price.\n","\n","Conclusion:\n","Overall, the panda plush toy is an excellent gift option for children and adults who love cute and cuddly toys. Despite its small size and asymmetrical design, the toy's softness and cuteness make up for its shortcomings. I highly recommend this product to anyone looking for a versatile and adorable gift option.\n","\n","\n","Here is the corrected and proofread version of the review in markdown format:\n","\n","```\n","Got this for my daughter for her birthday, because she keeps taking mine from my room.  Yes, adults also like pandas too.  She takes it everywhere with her, and it's super soft and cute.  One of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical.  It's a bit small for what I paid for it though.  I think there might be other options that are bigger for the same price.  It arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n","```\n"]}],"source":["prompt = f\"\"\"\n","proofread and correct this review. Make it more compelling. \n","Ensure it follows APA style guide and targets an advanced reader. \n","Output in markdown format.\n","Text: ```{text}```\n","\"\"\"\n","response_openai = get_completion(prompt)\n","print(response_openai)\n","\n","print(\"\\n\")\n","\n","response_cohere = cohere_complete(prompt)\n","print(response_cohere)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KDwVds63ulzu"},"source":["## Try it yourself!\n","Try changing the instructions to form your own review."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TlQmqDiJwte8"},"source":["Thanks to the following sites:\n","\n","https://writingprompts.com/bad-grammar-examples/\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOf6ShPZ03N37uGsgRSPYkB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
