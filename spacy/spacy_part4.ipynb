{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "\n",
    "In questa parte vengono introdotti i modelli statistici di spacy e a come personalizzarli per le nostre esigenze\n",
    "\n",
    "## Training and updating a model\n",
    "\n",
    "Ci focalizziamo in particolare sulla NER. Le domande a cui risponde questo paragrafo sono *\"Perchè personalizzare un modello?\"* e *\"Perchè non ci possiamo accontentare di un modello pre-addestrato?\"*.\n",
    "\n",
    "I modelli pre-addestrati hanno visto esempi generalisti, con la specializzazione li possiamo rendere più performanti sul nostro dominio. Questo è fondamentale e semplice quando si tratta di NER, un pò più critico se parliamo di POS-tagging e dependency parsing.\n",
    "\n",
    "Gli step dell'addestramento sono:\n",
    "\n",
    "* **inizializza** il modello con pesi random\n",
    "* **predici** alcuni esempi con i pesi attuali (usando `nlp.update`)\n",
    "* **compara** le previsioni con le etichette reali\n",
    "* **calcola** come modificare i pesi per migliorare le previsioni\n",
    "* **aggiorna** i pesi\n",
    "* torna alla 2 e riparti\n",
    "\n",
    "<br>\n",
    "\n",
    "![training](https://course.spacy.io/training.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Esempio\n",
    "\n",
    "L'entity recognition prende il testo e ne predice frasi e label nel contesto, quindi per addestrare questi modelli bisogna avere:\n",
    "* il testo\n",
    "* le entità\n",
    "* le etichette delle entità\n",
    "\n",
    "Ogni token può fa parte di una sola entità, non possono esserci sovrapposizioni. Per mostrare le entità in SpaCy si possono usare `Doc` e `Span`, è anche molto importante mostrare al modello i termini che non sono entità.\n",
    "Il nostro obiettivo è insegnare al modello a riconoscere nuove entità in contesti simili, anche se non erano presenti nei dati di addestramento.\n",
    "\n",
    "## Volume di dati\n",
    "\n",
    "Se si vuole aggiornare un modello pre-addestrato possono bastare centinaia o migliaia di esempi.\n",
    "\n",
    "Se si vuole addestrare una nuova categoria ci vorranno milioni di esempi, ad esempio la pipeline di SpaCy per l'inglese è stata addestrata su due milioni di parole con etichette per NER, POS-tagging e dependency parsing.\n",
    "\n",
    "Le etichette sono spesso create manualmente ma SpaCy forniscge il `Matcher` che ci aiuta in una creazione semi automatica.\n",
    "\n",
    "Ci sono tre tipi di dati:\n",
    "* **training**: per addestrare il modello\n",
    "* **evaluation**: mai visti prima dal modello, servono per misurarne le performance\n",
    "* **test**: mai visti dal modello ci aiutano a capire come si comporterebbe nel mondo reale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin, span\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a Doc with entity spans\n",
    "doc1 = nlp(\"iPhone X is coming\")\n",
    "doc1.ents = [span.Span(doc1, 0, 2, label=\"GADGET\")]\n",
    "# Create another doc without entity spans\n",
    "doc2 = nlp(\"I need a new phone! Any tips?\")\n",
    "\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# DocBin: container to efficiently store and save Doc objects\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il comando convert di spaCy converte automaticamente questi file nel formato binario di spaCy. Converte anche i file JSON nel vecchio formato utilizzato da spaCy v2.\n",
    "\n",
    "```bash\n",
    "$ python -m spacy convert ./train.gold.conll ./corpus\n",
    "```\n",
    "\n",
    "## Configuring and running training\n",
    "\n",
    "### Config\n",
    "\n",
    "Una volta creati i dataset si può passare alla fase di addestramento. In questa fase SpaCy usa un file di configurazione `config.cfg` dove vengono indicati la pipeline di nlp con i suoi componenti e tutti gli hyperparametri. Questa modalità è utile anche per rendere riproducibile la pipeline di addestramento. Un esempio di config file:\n",
    "\n",
    "```\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\", \"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[nlp.tokenizer]\n",
    "@tokenizers = \"spacy.Tokenizer.v1\"\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "hidden_width = 64\n",
    "# And so on...\n",
    "```\n",
    "\n",
    "Se volessimo usare qualche funzione custom nella nostra pipeline possiamo inserirla utilizzando il decorator *@*.\n",
    "\n",
    "Se non vogliamo partire da zero a creare il config file SpaCy ha una funzione che lo crea per noi:\n",
    "\n",
    "```bash\n",
    "$ python -m spacy init config ./config.cfg --lang en --pipeline ner\n",
    "```\n",
    "\n",
    "le componenti sono:\n",
    "* `init config` per far partire i comando\n",
    "* `./config.cfg` il file da generare\n",
    "* `--lang en` la lingua del modello\n",
    "* `--pipeline ner` le componenti della pipeline\n",
    "\n",
    "\n",
    "Il config file può essere creato anche tramite il [quickstart widget](https://spacy.io/usage/training#quickstart).\n",
    "\n",
    "> per visualizzare il file appena creato possiamo usare il comando `$ cat ./config.cfg`\n",
    "\n",
    "### Run\n",
    "\n",
    "Avendo la pipeline possiamo addestrare il modello col seguente comando:\n",
    "\n",
    "```bash\n",
    "$ python -m spacy train ./config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy\n",
    "```\n",
    "\n",
    "* `train` il comando da eseguire\n",
    "* `./config.cfg` la pipeline da utilizzare\n",
    "* `--output ./output` la pipeline addestrata\n",
    "* `--paths.train train.spacy`, `--paths.dev dev.spacy` i dataset da utilizzare\n",
    "\n",
    "Di seguito un'immagine di esempio della pipeline in train:\n",
    "\n",
    "<br>\n",
    "\n",
    "![train](https://miro.medium.com/v2/resize:fit:695/0*GR3Ibv_u4qH-tb9t.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "* nella prima colonna le epoche di apprendimento\n",
    "* nella seconda colonna ogni quanti esempi il modello fa un check\n",
    "* nell'ultima l'accuratezza del modello\n",
    "\n",
    "> L'addestramento viene eseguito finché il modello non smette di migliorare ed esce automaticamente.\n",
    "\n",
    "### Load\n",
    "\n",
    "Alla fine dell'addestramento vengono salvate due pipeline:\n",
    "\n",
    "* l'ultima epoca di apprendimento `model-last`\n",
    "* la migliore epoca di apprendimento `model-best`\n",
    "\n",
    "Queste pipeline possono essere caricate normalmente come quando usiamo `spacy.load()` per `en_core_web_sm`.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"/path/to/output/model-best\")\n",
    "doc = nlp(\"iPhone 11 vs iPhone 8: What's the difference?\")\n",
    "print(doc.ents)\n",
    "```\n",
    "\n",
    "### Deploy\n",
    "\n",
    "Per rendere più semplice portare in produzione o codividere le pipeline personalizzate SpaCy mette a dispsizione il modulo `package` che permette di creare un pacchetto `.tar.gz` installabile e utilizzabile. Per creare il pacchetto:\n",
    "\n",
    "```bash\n",
    "$ python -m spacy package /path/to/output/model-best ./packages --name my_pipeline --version 1.0.0\n",
    "```\n",
    "\n",
    "per installare il pacchetto nel nostro environment\n",
    "\n",
    "```bash\n",
    "$ cd ./packages/en_my_pipeline-1.0.0\n",
    "$ pip install dist/en_my_pipeline-1.0.0.tar.gz\n",
    "```\n",
    "\n",
    "per utilizzarlo in spacy\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_my_pipeline\")\n",
    "```\n",
    "\n",
    "### Best practices\n",
    "\n",
    "Di seguito alcuni problemi in cui si può incorrere allenando un modello pre-addestrato\n",
    "\n",
    "#### Catastrophic forgetting problem\n",
    "\n",
    "Se addestro un modello pre-addestrato con tanti nuovi esempi di una particolare etichetta potrebbe *dimenticare* qualcosa che aveva imparato prima. La *soluzione* è di non verticalizzare eccessivamente il dataset *specializzato* su cui si raffina il modello ma includere anche altre etichette che c'erano in quello generalista.\n",
    "\n",
    "#### Local Context\n",
    "\n",
    "Altro problema è quello di specializzare troppo le etichette, ad esempio `ADULT_CLOATHING` e `CHILDREN_CLOATHING` potrebbero essere delle etichette molto specializzate che il modello non riesce a distinguere, meglio un generico `CLOATHING`. Successivamente si possono specializzare mediante delle regole."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
