{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "## Installazione e primi passi\n",
    "\n",
    "Per installare **spacy** come prima cosa vanno installate le librerie *setuptools* e *wheel* perchè la libreria è disponibile come binary package.\n",
    "\n",
    "```\n",
    "pip install -U setuptools wheel\n",
    "pip install -U spacy\n",
    "```\n",
    "\n",
    "Al centro della libreria c'è un oggetto *pipeline* che solitamente viene inizializzato usando il nome **nlp**.\n",
    "\n",
    "Nella prossima cella inizializziamo una pipeline vuota per l'inglese col comando `spacy.blank(\"en\")`, con questa variabile possiamo eseguere diversi compiti per l'analisi del testo perchè contiene già diverse funzioni della pipeline e le regole per la lingua scelta.\n",
    "\n",
    "> Per cambiare lingua, ad esempio tedesco o italiano basterà usare `spacy.blank(\"de\")` o `spacy.blank(\"it\")`\n",
    "\n",
    "<br>\n",
    "\n",
    "![spacy_pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Nel prossimo esempio eseguiamo una semplice tokenizzazione del testo. Un token può essere una parola o la punteggiatura nel testo, per estrarre un token dal testo basta dare la sua posizione. L'oggetto *Token* di spacy dà accesso a diverse informazioni sui token, ad esempio la forma testuale dello stesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "![token](https://course.spacy.io/doc.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n",
      "\n",
      "\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(doc[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno *span* non è altro che una parte di documento che consiste in uno o più token. Per creare uno span basta utilizzare la classica notazione python per selezionare parti di una stringa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "\n",
    "print(span)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per vedere gli attributi in una frase si possono usare gli attributi di spacy, ad esempio:\n",
    "* `is_alpha` indica i token caratteri alfabetici\n",
    "* `is_punct` la punteggiatura\n",
    "* `like_num` i numeri, ma non solo se scritti sotto forma di cifre, anche come parola\n",
    "\n",
    "Questi sono chiamati ***attributi lessicali*** dipendono dal dizionario utilizzato e non dal contesto della frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  0\n",
      "text:  It\n",
      "alpha:  True\n",
      "punct:  False\n",
      "num:  False\n",
      "\n",
      "\n",
      "index:  1\n",
      "text:  coasts\n",
      "alpha:  True\n",
      "punct:  False\n",
      "num:  False\n",
      "\n",
      "\n",
      "index:  2\n",
      "text:  $\n",
      "alpha:  False\n",
      "punct:  False\n",
      "num:  False\n",
      "\n",
      "\n",
      "index:  3\n",
      "text:  5\n",
      "alpha:  False\n",
      "punct:  False\n",
      "num:  True\n",
      "\n",
      "\n",
      "index:  4\n",
      "text:  .\n",
      "alpha:  False\n",
      "punct:  True\n",
      "num:  False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It coasts $5 .\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"index: \", token.i)\n",
    "    print(\"text: \", token.text)\n",
    "    print(\"alpha: \", token.is_alpha)\n",
    "    print(\"punct: \", token.is_punct)\n",
    "    print(\"num: \", token.like_num)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline addestrate\n",
    "\n",
    "Una pipeline addestrata è un oggetto SpaCy è un modello che è capace di estrarre attributi da un contesto (POS, NER, ...), è stato già addestrato su dati etichettati e può essere raffinato (fine-tuned) utilizzando nuovi dati.\n",
    "\n",
    "<br>\n",
    "\n",
    "![pos-tags](https://miro.medium.com/max/940/1*m2qeNjOSiDZzTFhdHpORqw.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "SpaCy ha diverse pipeline pre-addestrate che possono essere scaricate mediante il comando `spacy.load`, ad esempio `en_core_web_sm` è una pipeline small, in inglese addestrata su testi presi dal web. Quando vengono scaricate le pipeline si scaricano i pesi per fare inferenza, i metadati sulla pipeline, il vocabolario e il file di configurazione su come è stata addestrata la pipeline.\n",
    "\n",
    "Il primo esempio che vedremo è POS-tags utilizzando un modello preaddestrato. In questo caso oltre al testo ci facciamo restituire l'attibuto *pos_*.\n",
    "\n",
    "> `pos_` ci restituisce la forma testuale, `pos` senza underscore restituisce l'ID del tag. In SpaCy questa è la convenzione per nominare degli attributi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 95 PRON\n",
      "hate 100 VERB\n",
      "pizza 92 NOUN\n",
      "with 85 ADP\n",
      "pineapple 92 NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"I hate pizza with pineapple\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos, token.pos_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oltre al POS-tags basico SpaCy restituisce anche le relazioni tra le parole, ad esempio se un nome è il soggetto o l'oggetto della frase o a chi/cosa si riferisce un articolo o un avverbio.\n",
    "\n",
    "Nel nostro caso avremo:\n",
    "* I: *nsubj* nominal subject\n",
    "* pizza: *dobj* directed object\n",
    "* pineapple: *pobj* object for preposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj hate\n",
      "hate VERB ROOT hate\n",
      "pizza NOUN dobj hate\n",
      "with ADP prep hate\n",
      "pineapple NOUN pobj with\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di passare oltre andiamo a vedere cosa ha una pipeline preaddestrata sotto il cofano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere tra le componenti ci sono:\n",
    "* **tok2vec**: per trasformare i token in vettori numerici\n",
    "* **tagger**: per fare pos-tagging\n",
    "* **parser**: per assegnare le dependency label\n",
    "* **attribute_ruler**: mappa gli attributi dei token e assegna le eccezioni\n",
    "* **lemmatizer**: indica i lemmi dei token\n",
    "* **ner**: per fare name-entities-recognition\n",
    "\n",
    "Nel prossimo esempio vedremo proprio la NER (Name Entities Recongnition). Mediante questa pipeline si assegna un'etichetta alle parole, ad esempio *persone*, *organizzazioni*, ...\n",
    "\n",
    "La pipeline restituisce la NER tramite `.ents`, questo ritorna degli oggetti **Span** che a loro volta contengono testo ed etichette. Nel nostro esempio *Apple* *ORG* (organization) e *Cupertino* *GPE* (geopolitical entities).\n",
    "\n",
    "> Nel secondo esempio abbiamo sostituito *Cupertino* con un più generico *Silicon Valley* notate che il tag cambia da *GPE*  a *LOC* (location). Possiamo capire perchè usando il comando `spacy.explain(\"GPE\")` e `spacy.explain(\"LOC\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Cupertino GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple's headquarters is in Cupertino\")\n",
    "\n",
    "#doc = nlp(\"Apple's headquarters is in Silicon Valley\")\n",
    "\n",
    "#doc = nlp(\"Peter buys it on Amazon for 1$\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si possono visualizzare i risultati di una NER utilizzando la libreria *displacy*. Questa libreria ha due metodi in particolare `displacy.serve()` per avviare un web server e `displacy.render` per un markup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "'s headquarters is in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cupertino\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based match\n",
    "\n",
    "Come scrivere delle regole per trovare parole e frasi nel testo non limitandosi alle espressioni regolari (*regex*).\n",
    "Le regex si limitano a trovare stringhe, con questi oggetti invece possiamo cercare anche documenti e token, questo lo rende molto più flessibile.\n",
    "\n",
    "I modelli di corrispondenza sono liste di dizionari, ogni dizionario è un token, la chiave è il nome del token `[{\"TEXT\": \"iPhone\"}, {\"TEXT\":\"X\"}]`. Si possono creare nuovi attributi mediante i token ad esempio `[{\"LOWER\":\"iphone\"}, {\"LOWER\":\"x\"}]`.\n",
    "\n",
    "Per usarele corrispondenze (pattern) dobbiamo importare l'oggetto *Matcher* da spacy, creare un *oggetto nlp* caricando una pipeline e inizializzare il *Matcher* con il vocabolario della pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo `matcher.add` consente di aggiungere dei pattern, il primo elemento nel metodo è l'ID del pattern (una stringa), il secondo è il pattern (và passato come lista).\n",
    "\n",
    "Per usare questo oggetto bisogna richiamarlo sul testo. Per visualizzare il risultato creiamo uno span utilizzando i seguenti elementi del matcher:\n",
    "* match_id: hash value del pattern name\n",
    "* start: indice iniziale del matched span\n",
    "* end: indice finale del matched span\n",
    "\n",
    "> il matcher ritorna liste di tuple, le tuple sono da tre elementi *[match_id, start, end]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"TEXT\":\"iPhone\"}, {\"TEXT\":\"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"Peter buys an iPhone X on Amazon for 100$\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come utilizzando i *Matcher* possiamo creare nuovi pattern, ad esempio il seguente basato sul verbo *love* (nelle sue declinazioni) seguito da un nome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n",
      "loves turtles\n"
     ]
    }
   ],
   "source": [
    "love_pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"LOVE_PATTERN\", [love_pattern])\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more. Peter loves turtles\")\n",
    "\n",
    "love_matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in love_matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel prossimo esempio c'è un pattern che cerca un aggettivo seguito da un nome, e da un secondo nome che però è opzionale. Per creare questo pattern usiamo la combinazione ***\"OP\":\"?\"***.\n",
    "\n",
    "> Per maggiori informazioni sul rule-based matching consultare la [documentazione](https://spacy.io/usage/rule-based-matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7311948a7550bce7c0db859b04b0f1855317afbfa5ee8d2f1ff81230a0dc77ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
